{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqf02HYqqDJ3"
      },
      "outputs": [],
      "source": [
        "NEO4j_URL = \"\"\n",
        "NEO4J_USERNAME = \"\"\n",
        "NEO4J_PASSWORD = \"\"\n",
        "GITHUB_TOKEN = \"\"\n",
        "GITHUB_BASE_URL = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BiZM2_t3qkXY"
      },
      "outputs": [],
      "source": [
        "!pip install -q graphiti-core langchain-openai langgraph langchain-openai langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnPEmYUhqqiy",
        "outputId": "3b56d4e6-718a-41ee-f3f6-00a30cdf6b90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "from contextlib import suppress\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import Annotated\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Image, display\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWV8mq_UqrO0"
      },
      "outputs": [],
      "source": [
        "# Note: This will clear the database\n",
        "# await clear_data(client.driver)\n",
        "# await client.build_indices_and_constraints()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMhSNPQSrbFO"
      },
      "source": [
        "<h1>Create client for graphiti </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Da43gn5Yrah7"
      },
      "outputs": [],
      "source": [
        "from graphiti_core.llm_client.openai_client import OpenAIClient\n",
        "from openai import OpenAI\n",
        "\n",
        "from graphiti_core import Graphiti\n",
        "from graphiti_core.llm_client.config import LLMConfig\n",
        "from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\n",
        "from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "from graphiti_core.llm_client.openai_client import OpenAIClient\n",
        "from graphiti_core.llm_client.config import LLMConfig\n",
        "\n",
        "\n",
        "openai_api = AsyncOpenAI(\n",
        "    api_key=GITHUB_TOKEN,\n",
        "    base_url=GITHUB_BASE_URL,\n",
        ")\n",
        "\n",
        "llm_config = LLMConfig(\n",
        "    api_key=GITHUB_TOKEN,\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    small_model=\"gpt-4.1-nano\",\n",
        "    base_url=GITHUB_BASE_URL,\n",
        ")\n",
        "\n",
        "llm_client = OpenAIClient(client=openai_api, config=llm_config)\n",
        "embedder_config = OpenAIEmbedderConfig(\n",
        "    api_key=GITHUB_TOKEN,\n",
        "    base_url=GITHUB_BASE_URL,\n",
        "\n",
        "    embedding_model=\"text-embedding-3-small\",\n",
        "    embedding_dim=1536,\n",
        ")\n",
        "\n",
        "\n",
        "client = Graphiti(\n",
        "    NEO4j_URL,\n",
        "    NEO4J_USERNAME,\n",
        "    NEO4J_PASSWORD,\n",
        "\n",
        "    llm_client=llm_client,\n",
        "    embedder=OpenAIEmbedder(config=embedder_config),\n",
        "\n",
        "    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHokGXqBqvKU"
      },
      "source": [
        "<h1>Test create graph base on chat message</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKY6G9yNq4mC"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from datetime import datetime\n",
        "# from graphiti_core import GraphitiClient\n",
        "from graphiti_core.nodes import EpisodeType\n",
        "\n",
        "# The conversation stream\n",
        "conversation_turns = [\n",
        "\"\"\"\n",
        "Officer: Where were you yesterday evening?\n",
        "John: I was at the City Mall, near the food court.\n",
        "\"\"\"\n",
        ",\"\"\"\n",
        "Officer: What time were you there?\n",
        "John: I arrived at 6:30 p.m. and left around 8:00 p.m.\n",
        "\n",
        "Officer: What were you doing at that time?\n",
        "John: I was having dinner and meeting a friend.\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "Officer: Did you go anywhere else after that?\n",
        "John: Yes, I went to a coffee shop on Green Street at 8:15 p.m.\n",
        "\n",
        "Officer: What action did you take there?\n",
        "John: I sat down, drank coffee, and checked my phone until 9:30 p.m.\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "async def run_realtime_interview(client , data ):\n",
        "\n",
        "  # 1. Ingest the single turn immediately\n",
        "  await client.add_episode(\n",
        "      name=f\"Interview_Segment_{data['index']}\",\n",
        "      episode_body=data['content'],\n",
        "      source_description=\"Real-time interview chat logs\",\n",
        "      source=EpisodeType.message,\n",
        "      reference_time=datetime.now() # Current time allows Graphiti to order events\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ESlWN8uq6GP"
      },
      "outputs": [],
      "source": [
        "id = 0\n",
        "data = {\n",
        "    \"index\" : id,\n",
        "    \"content\" : conversation_turns[id]\n",
        "}\n",
        "await run_realtime_interview(client , data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG5LGNdZq73H"
      },
      "source": [
        "<h1>Test query</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJls-Rtvq7H0",
        "outputId": "05097ac9-6a95-41b7-f7a7-3aad4389bfab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node: City Mall is a location where John stated he was present near the food court yesterday evening.\n",
            "Node: John was at the City Mall near the food court yesterday evening from 6:30 p.m. to 8:00 p.m., having dinner and meeting a friend.\n",
            "Node: The food court is a location at the City Mall where John mentioned being present yesterday evening.\n",
            "Node: John met a friend for dinner at the City Mall food court on the evening of the incident, arriving at 6:30 p.m. and leaving around 8:00 p.m.\n",
            "Node: Officer questioned John about his whereabouts on a specific evening; John stated he was at the City Mall near the food court from 6:30 p.m. to 8:00 p.m., having dinner and meeting a friend.\n",
            "Edge: John was at the City Mall.\n",
            "Edge: The City Mall contains the food court.\n",
            "Edge: John met with a friend.\n"
          ]
        }
      ],
      "source": [
        "from graphiti_core.search.search_config_recipes import COMBINED_HYBRID_SEARCH_CROSS_ENCODER\n",
        "\n",
        "# G√°n c·∫•u h√¨nh t√¨m ki·∫øm n√†y\n",
        "search_config = COMBINED_HYBRID_SEARCH_CROSS_ENCODER.model_copy(deep=True)\n",
        "\n",
        "# G·ªçi _search() v·ªõi config\n",
        "results = await client._search(\n",
        "    query=\"Did John have anything with him when he was at city mall last night?\",\n",
        "    config=search_config,\n",
        ")\n",
        "\n",
        "# X·ª≠ l√Ω k·∫øt qu·∫£\n",
        "for node in results.nodes:\n",
        "    print(\"Node:\", node.summary)\n",
        "\n",
        "for edge in results.edges:\n",
        "    print(\"Edge:\", edge.fact)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQy4-mmIrLaw"
      },
      "source": [
        "<h1>Implement HyDe Rag</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aHJry2G6rR1Y"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=GITHUB_BASE_URL,\n",
        "    api_key=GITHUB_TOKEN,\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "\n",
        "hyde_template = \"\"\"Please write a short, plausible passage that answers the question.\n",
        "It doesn't have to be factually true, but it should look like a factual statement found in a database.\n",
        "\n",
        "Question: {question}\n",
        "Hypothetical Answer:\"\"\"\n",
        "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)\n",
        "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "async def retrieve_strict_raw_edge(hypothetical_answer: str):\n",
        "    print(f\"üîé ƒêang query Graphiti (HyDE: {hypothetical_answer}\")\n",
        "\n",
        "    from graphiti_core.search.search_config_recipes import COMBINED_HYBRID_SEARCH_CROSS_ENCODER\n",
        "\n",
        "    search_config = COMBINED_HYBRID_SEARCH_CROSS_ENCODER.model_copy(deep=True)\n",
        "\n",
        "    results = await client._search(\n",
        "        query=hypothetical_answer,\n",
        "        config=search_config,\n",
        "    )\n",
        "\n",
        "    for node in results.nodes:\n",
        "        print(\"Node:\", node.summary)\n",
        "\n",
        "    # for edge in results.edges:\n",
        "    #     print(\"Edge:\", edge.fact)\n",
        "\n",
        "    if not results:\n",
        "        return \"No relevant facts found in database.\"\n",
        "\n",
        "    context = \"\"\n",
        "    for node in results.nodes:\n",
        "      context += node.summary + \"\\n\"\n",
        "    for edge in results.edges:\n",
        "      context += edge.fact + \"\\n\"\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "final_template = \"\"\"You are an expert Temporal Knowledge Graph Analyst.\n",
        "Your job is to answer the user's question based STRICTLY on the provided \"FACT FACTS\".\n",
        "\n",
        "### REAL FACTS FROM DATABASE:\n",
        "{context}\n",
        "\n",
        "### USER QUESTION:\n",
        "{question}\n",
        "\n",
        "### ANSWER:\"\"\"\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_template(final_template)\n",
        "\n",
        "full_chain = (\n",
        "    {\n",
        "\n",
        "        \"context\": hyde_chain | RunnableLambda(retrieve_strict_raw_edge),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | final_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMmMihbGseVz"
      },
      "source": [
        "<h1>Test Query</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq1eAFJbsi_I",
        "outputId": "728e15e0-9a46-420b-b6b2-6c489ee3a63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User h·ªèi: Did John have anything with him when he was at city mall last night?\n",
            "------------------------------\n",
            "üîé ƒêang query Graphiti (HyDE: Yes, John had his wallet and a shopping bag with him when he was at the city mall last night.\n",
            "Node: City Mall is a location where John stated he was present near the food court yesterday evening.\n",
            "Node: John was at the City Mall near the food court yesterday evening from 6:30 p.m. to 8:00 p.m., having dinner and meeting a friend.\n",
            "Node: The food court is a location at the City Mall where John mentioned being present yesterday evening.\n",
            "Node: Officer questioned John about his whereabouts on a specific evening; John stated he was at the City Mall near the food court from 6:30 p.m. to 8:00 p.m., having dinner and meeting a friend.\n",
            "Node: John met a friend for dinner at the City Mall food court on the evening of the incident, arriving at 6:30 p.m. and leaving around 8:00 p.m.\n",
            "------------------------------\n",
            "ü§ñ AI Tr·∫£ l·ªùi cu·ªëi c√πng:\n",
            "Based on the provided facts, there is no information indicating that John had anything with him when he was at the City Mall last night.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 5. CH·∫†Y TH·ª¨ ---\n",
        "async def main():\n",
        "    user_q = \"Did John have anything with him when he was at city mall last night?\"\n",
        "\n",
        "    print(f\"User h·ªèi: {user_q}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # G·ªçi chain\n",
        "    response = await full_chain.ainvoke(user_q)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(\"ü§ñ AI Tr·∫£ l·ªùi cu·ªëi c√πng:\")\n",
        "    print(response)\n",
        "\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
